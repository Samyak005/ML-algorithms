{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y45yhDufI5Uo"
      },
      "source": [
        "# **RNN**\n",
        "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55i8db8uI5U3"
      },
      "source": [
        "IMDB sentiment classification task\n",
        "\n",
        "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.\n",
        "\n",
        "You can download the dataset from http://ai.stanford.edu/~amaas/data/sentiment/  or you can directly use \n",
        "\" from keras.datasets import imdb \" to import the dataset.\n",
        "\n",
        "Few points to be noted:\n",
        "Modules like SimpleRNN, LSTM, Activation layers, Dense layers, Dropout can be directly used from keras\n",
        "For preprocessing, you can use required "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SozhvLNkI5U6",
        "outputId": "930d7630-ae57-4631-d5c7-9ba760b5f732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ]
        }
      ],
      "source": [
        "#load the imdb dataset \n",
        "from keras.datasets import imdb\n",
        "\n",
        "vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrilwfurI5VA",
        "outputId": "a88860cb-7521-477f-dc1d-d8e8bfba9987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---review---\n",
            "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "---review with words---\n",
            "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
            "---label---\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "#the review is stored as a sequence of integers. \n",
        "# These are word IDs that have been pre-assigned to individual words, and the label is an integer\n",
        "\n",
        "print('---review---')\n",
        "print(X_train[6])\n",
        "print('---label---')\n",
        "print(y_train[6])\n",
        "\n",
        "# to get the actual review\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in X_train[6]])\n",
        "print('---label---')\n",
        "print(y_train[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h6upWxEWI5VC"
      },
      "outputs": [],
      "source": [
        "#pad sequences (write your code here)\n",
        "from keras.preprocessing import sequence\n",
        "max_words = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RcCOpeNI5VF",
        "outputId": "9a7be441-52cd-4344-ab50-505e02c251a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 32)           160000    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 100)               13300     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 173,401\n",
            "Trainable params: 173,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#design a RNN model (write your code)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
        "\n",
        "embedding_size=32\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model_rnn.add(SimpleRNN(100))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "print(model_rnn.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "InQ2TED3I5VI"
      },
      "outputs": [],
      "source": [
        "#train and evaluate your model\n",
        "#choose your loss function and optimizer and mention the reason to choose that particular loss function and optimizer\n",
        "# use accuracy as the evaluation metric\n",
        "\n",
        "model_rnn.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Binary cross entropy is chosen due to the nature of the problem i.e. binary classification. Adam optimiser has been proven to be among the best optimisers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6A9Q0xmI5VJ",
        "outputId": "9d21bce7-b44e-414d-e3b0-5810a45dfe96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "390/390 [==============================] - 245s 619ms/step - loss: 0.6394 - accuracy: 0.6115 - val_loss: 0.5671 - val_accuracy: 0.7031\n",
            "Epoch 2/3\n",
            "390/390 [==============================] - 242s 619ms/step - loss: 0.5718 - accuracy: 0.7060 - val_loss: 0.5284 - val_accuracy: 0.7344\n",
            "Epoch 3/3\n",
            "390/390 [==============================] - 241s 618ms/step - loss: 0.5329 - accuracy: 0.7273 - val_loss: 0.6801 - val_accuracy: 0.6094\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f83c7568fd0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "model_rnn.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTXG__EmI5VM",
        "outputId": "e66046d6-ab84-487c-d211-400c75986adb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.6280800104141235\n"
          ]
        }
      ],
      "source": [
        "#evaluate the model using model.evaluate()\n",
        "scores = model_rnn.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1uSo8DgI5VN"
      },
      "source": [
        "# **LSTM**\n",
        "\n",
        "Instead of using a RNN, now try using a LSTM model and compare both of them. Which of those performed better and why ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk4rLYHwI5VP",
        "outputId": "0a429dd9-e54a-44b1-c75b-63bc017bb452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 500, 32)           160000    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               53200     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "390/390 [==============================] - 35s 78ms/step - loss: 0.4554 - accuracy: 0.7780 - val_loss: 0.2900 - val_accuracy: 0.8750\n",
            "Epoch 2/3\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.2883 - accuracy: 0.8851 - val_loss: 0.2262 - val_accuracy: 0.9062\n",
            "Epoch 3/3\n",
            "390/390 [==============================] - 30s 76ms/step - loss: 0.2455 - accuracy: 0.9058 - val_loss: 0.2438 - val_accuracy: 0.9219\n",
            "Test accuracy: 0.8669999837875366\n"
          ]
        }
      ],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
        "\n",
        "embedding_size=32\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model_lstm.add(LSTM(100))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "print(model_lstm.summary())\n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "model_lstm.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "scores = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7cYYz3dzrpe"
      },
      "source": [
        "LSTM performs considerably better than RNN. 62.8% test accuracy in RNN vs 86.69% test accuracy in LSTM. This was expected as LSTM solves the problem of vanishing and exploding gradients(or long range dependencies) and consists of 3 gates whereas vanilla RNN has just 1 gate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBtRY9jmI5VQ"
      },
      "source": [
        "Perform Error analysis and explain using few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nrs2ylDaI5VR"
      },
      "outputs": [],
      "source": [
        "# for user prediction\n",
        "def user_input_processing(model, review):\n",
        "    vec = []\n",
        "    for word in review.split(\" \"):\n",
        "        if word[-1] == \".\":\n",
        "            word = word[:-1]\n",
        "        vec.append(word2id[str.lower(word)])\n",
        "    vec_padded = sequence.pad_sequences([vec], 500)\n",
        "    print(review, model.predict(vec_padded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMIV7rfFzoqX",
        "outputId": "4f6b434b-9ad3-4a9b-e518-bc07e918bd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One of the finest films made in recent years. [[0.56504375]]\n"
          ]
        }
      ],
      "source": [
        "user_input_processing(model_rnn, \"One of the finest films made in recent years.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R1yHfZb0ZrK",
        "outputId": "e2084117-3480-46d4-aa86-9b41f34b054f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictable and bad. The acting was terrible and the story was common. [[0.3047255]]\n"
          ]
        }
      ],
      "source": [
        "user_input_processing(model_rnn, \"Predictable and bad. The acting was terrible and the story was common.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEFcDT0R0sPp",
        "outputId": "5efd2322-76e3-46a3-b7cd-f76bf93e2368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One of the finest films made in recent years. [[0.8364623]]\n"
          ]
        }
      ],
      "source": [
        "user_input_processing(model_lstm, \"One of the finest films made in recent years.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBUExQjnDcUm",
        "outputId": "da42859f-572b-460e-ca3e-d5fccacf371b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictable and bad. The acting was terrible and the story was common. [[0.5422326]]\n"
          ]
        }
      ],
      "source": [
        "user_input_processing(model_lstm, \"Predictable and bad. The acting was terrible and the story was common.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSaDQs6VDgz_"
      },
      "source": [
        "On the positive review \"One of the finest films made in recent years.\", LSTM gives higher score than RNN but on the negative review RNN has a lower score than LSTM. This can be due to the model being trained on very small data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
